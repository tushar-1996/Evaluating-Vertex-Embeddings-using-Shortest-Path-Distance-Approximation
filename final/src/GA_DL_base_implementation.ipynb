{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GA_DL_base_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1FvsDnbHMbzW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "580AKb_bZuAI"
      },
      "source": [
        "## Source Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jznFsNMVEezo",
        "outputId": "06b68ca8-2e15-4a0b-d167-9cba302dda9d"
      },
      "source": [
        "import gdown\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import networkx as nx\n",
        "import csv\n",
        "\n",
        "!mkdir datasets\n",
        "!pip install pyvis\n",
        "from pyvis.network import Network\n",
        "\n",
        "#############################################################################################\n",
        "# Facebook DATASET DOWNLOADS\n",
        "#############################################################################################\n",
        "\n",
        "# Download facebook dataset edgelist in txt format (extracted from mtx)\n",
        "!gdown https://drive.google.com/uc?id=1v03XWRternGLDpRfKbRGoMiVX3dpOW3G -O datasets/facebook_edges.txt\n",
        "\n",
        "\n",
        "# Download pretrained facebook HARP embeddings\n",
        "!gdown https://drive.google.com/uc?id=1415v_BZOgVijs40gPAoIA6j4JZ6f0ogM -O datasets/Facebook_HARP_deepwalk.npy\n",
        "!gdown https://drive.google.com/uc?id=1V9I72BFDYBv3LNF6DLyK5Coo0XKkU5ex -O datasets/Facebook_HARP_node2vec.npy\n",
        "!gdown https://drive.google.com/uc?id=1Ziaz4wWqcWPHqgc9gLwxbP6RptO5eSZy -O datasets/Facebook_HARP_line.npy\n",
        "\n",
        "#############################################################################################\n",
        "# Youtube DATASET DOWNLOADS\n",
        "#############################################################################################\n",
        "# Download youtube dataset edgelist in cvs format\n",
        "!gdown https://drive.google.com/uc?id=12aGrbOZqVMfOP46X8lj5qwqQui4kbMjZ -O datasets/youtube_edges.csv\n",
        "\n",
        "#############################################################################################\n",
        "# Douban DATASET DOWNLOADS source: http://datasets.syr.edu/pages/datasets.html\n",
        "#############################################################################################\n",
        "# Download douban dataset edgelist in cvs format\n",
        "!gdown https://drive.google.com/uc?id=1ssjgKF5WpiXcIk7DfF6BXwPoWkqr5rOS -O datasets/douban_edges.csv\n",
        "\n",
        "# FRAMEWORK SETUPS\n",
        "#############################################################################################\n",
        "# Graph embeddings framework @see: https://github.com/shenweichen/GraphEmbedding\n",
        "!gdown https://drive.google.com/uc?id=1QwaC2pz6wC8QGAA1N7208SxEzdPfkn3S -O GraphEmbedding.zip\n",
        "!unzip GraphEmbedding.zip\n",
        "# HARP\n",
        "!gdown https://drive.google.com/uc?id=174k2qDmDhXrKFivGD00jBWAvJp9b1kiq -O HARP.zip\n",
        "!unzip HARP.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyvis\n",
            "  Downloading pyvis-0.1.9-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.5.1)\n",
            "Collecting jsonpickle>=1.4.1\n",
            "  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from pyvis) (5.5.0)\n",
            "Requirement already satisfied: jinja2>=2.9.6 in /usr/local/lib/python3.7/dist-packages (from pyvis) (2.11.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (57.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (5.0.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.3.0->pyvis) (0.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.9.6->pyvis) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonpickle>=1.4.1->pyvis) (4.6.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.3.0->pyvis) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=5.3.0->pyvis) (0.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->jsonpickle>=1.4.1->pyvis) (3.7.4.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython>=5.3.0->pyvis) (0.7.0)\n",
            "Installing collected packages: jsonpickle, pyvis\n",
            "Successfully installed jsonpickle-2.0.0 pyvis-0.1.9\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1v03XWRternGLDpRfKbRGoMiVX3dpOW3G\n",
            "To: /content/datasets/facebook_edges.txt\n",
            "100% 854k/854k [00:00<00:00, 5.22MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1415v_BZOgVijs40gPAoIA6j4JZ6f0ogM\n",
            "To: /content/datasets/Facebook_HARP_deepwalk.npy\n",
            "100% 2.07M/2.07M [00:00<00:00, 9.66MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1V9I72BFDYBv3LNF6DLyK5Coo0XKkU5ex\n",
            "To: /content/datasets/Facebook_HARP_node2vec.npy\n",
            "100% 2.07M/2.07M [00:00<00:00, 9.69MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ziaz4wWqcWPHqgc9gLwxbP6RptO5eSZy\n",
            "To: /content/datasets/Facebook_HARP_line.npy\n",
            "100% 2.07M/2.07M [00:00<00:00, 9.66MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EcaUE7hyzTLybL-6oLrSiLzRXzlNLS1i\n",
            "To: /content/datasets/blogcatalog_edges.csv\n",
            "3.26MB [00:00, 15.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12aGrbOZqVMfOP46X8lj5qwqQui4kbMjZ\n",
            "To: /content/datasets/youtube_edges.csv\n",
            "38.7MB [00:01, 27.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QwaC2pz6wC8QGAA1N7208SxEzdPfkn3S\n",
            "To: /content/GraphEmbedding.zip\n",
            "100% 1.06M/1.06M [00:00<00:00, 4.94MB/s]\n",
            "Archive:  GraphEmbedding.zip\n",
            "   creating: GraphEmbedding/\n",
            "  inflating: __MACOSX/._GraphEmbedding  \n",
            "  inflating: GraphEmbedding/.DS_Store  \n",
            "  inflating: __MACOSX/GraphEmbedding/._.DS_Store  \n",
            "  inflating: GraphEmbedding/LICENSE  \n",
            "  inflating: __MACOSX/GraphEmbedding/._LICENSE  \n",
            "   creating: GraphEmbedding/pics/\n",
            "  inflating: __MACOSX/GraphEmbedding/._pics  \n",
            "  inflating: GraphEmbedding/README.md  \n",
            "  inflating: __MACOSX/GraphEmbedding/._README.md  \n",
            "  inflating: GraphEmbedding/setup.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/._setup.py  \n",
            "  inflating: GraphEmbedding/.gitignore  \n",
            "  inflating: __MACOSX/GraphEmbedding/._.gitignore  \n",
            "   creating: GraphEmbedding/examples/\n",
            "  inflating: __MACOSX/GraphEmbedding/._examples  \n",
            "   creating: GraphEmbedding/ge/\n",
            "  inflating: __MACOSX/GraphEmbedding/._ge  \n",
            "  inflating: GraphEmbedding/.gitattributes  \n",
            "  inflating: __MACOSX/GraphEmbedding/._.gitattributes  \n",
            "   creating: GraphEmbedding/.git/\n",
            "  inflating: __MACOSX/GraphEmbedding/._.git  \n",
            "   creating: GraphEmbedding/data/\n",
            "  inflating: __MACOSX/GraphEmbedding/._data  \n",
            "  inflating: GraphEmbedding/pics/edge_list.png  \n",
            "  inflating: __MACOSX/GraphEmbedding/pics/._edge_list.png  \n",
            "  inflating: GraphEmbedding/pics/code.png  \n",
            "  inflating: __MACOSX/GraphEmbedding/pics/._code.png  \n",
            "  inflating: GraphEmbedding/pics/deepctrbot.png  \n",
            "  inflating: __MACOSX/GraphEmbedding/pics/._deepctrbot.png  \n",
            "  inflating: GraphEmbedding/pics/weichennote.png  \n",
            "  inflating: __MACOSX/GraphEmbedding/pics/._weichennote.png  \n",
            "  inflating: GraphEmbedding/examples/node2vec_flight.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._node2vec_flight.py  \n",
            "  inflating: GraphEmbedding/examples/struc2vec_flight.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._struc2vec_flight.py  \n",
            "  inflating: GraphEmbedding/examples/line_wiki.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._line_wiki.py  \n",
            "  inflating: GraphEmbedding/examples/deepwalk_wiki.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._deepwalk_wiki.py  \n",
            "  inflating: GraphEmbedding/examples/alias.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._alias.py  \n",
            "  inflating: GraphEmbedding/examples/node2vec_wiki.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._node2vec_wiki.py  \n",
            "  inflating: GraphEmbedding/examples/sdne_wiki.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/examples/._sdne_wiki.py  \n",
            "  inflating: GraphEmbedding/ge/alias.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/._alias.py  \n",
            "  inflating: GraphEmbedding/ge/.DS_Store  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/._.DS_Store  \n",
            "  inflating: GraphEmbedding/ge/classify.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/._classify.py  \n",
            "  inflating: GraphEmbedding/ge/walker.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/._walker.py  \n",
            "  inflating: GraphEmbedding/ge/__init__.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/.___init__.py  \n",
            "   creating: GraphEmbedding/ge/models/\n",
            "  inflating: __MACOSX/GraphEmbedding/ge/._models  \n",
            "  inflating: GraphEmbedding/ge/utils.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/._utils.py  \n",
            "  inflating: GraphEmbedding/.git/config  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._config  \n",
            "   creating: GraphEmbedding/.git/objects/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._objects  \n",
            "  inflating: GraphEmbedding/.git/HEAD  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._HEAD  \n",
            "   creating: GraphEmbedding/.git/info/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._info  \n",
            "   creating: GraphEmbedding/.git/logs/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._logs  \n",
            "  inflating: GraphEmbedding/.git/description  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._description  \n",
            "   creating: GraphEmbedding/.git/hooks/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._hooks  \n",
            "   creating: GraphEmbedding/.git/refs/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._refs  \n",
            "  inflating: GraphEmbedding/.git/index  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._index  \n",
            "  inflating: GraphEmbedding/.git/packed-refs  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/._packed-refs  \n",
            "  inflating: GraphEmbedding/.git/FETCH_HEAD  \n",
            "   creating: GraphEmbedding/data/flight/\n",
            "  inflating: __MACOSX/GraphEmbedding/data/._flight  \n",
            "   creating: GraphEmbedding/data/wiki/\n",
            "  inflating: __MACOSX/GraphEmbedding/data/._wiki  \n",
            "  inflating: GraphEmbedding/ge/models/sdne.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/models/._sdne.py  \n",
            "  inflating: GraphEmbedding/ge/models/line.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/models/._line.py  \n",
            "  inflating: GraphEmbedding/ge/models/__init__.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/models/.___init__.py  \n",
            "  inflating: GraphEmbedding/ge/models/struc2vec.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/models/._struc2vec.py  \n",
            "  inflating: GraphEmbedding/ge/models/node2vec.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/models/._node2vec.py  \n",
            "  inflating: GraphEmbedding/ge/models/deepwalk.py  \n",
            "  inflating: __MACOSX/GraphEmbedding/ge/models/._deepwalk.py  \n",
            "   creating: GraphEmbedding/.git/objects/pack/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/objects/._pack  \n",
            "  inflating: GraphEmbedding/.git/info/exclude  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/info/._exclude  \n",
            "  inflating: GraphEmbedding/.git/logs/HEAD  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/._HEAD  \n",
            "   creating: GraphEmbedding/.git/logs/refs/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/._refs  \n",
            "  inflating: GraphEmbedding/.git/hooks/commit-msg.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._commit-msg.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/pre-rebase.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._pre-rebase.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/pre-commit.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._pre-commit.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._applypatch-msg.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._fsmonitor-watchman.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/pre-receive.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._pre-receive.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._prepare-commit-msg.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/post-update.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._post-update.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._pre-merge-commit.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._pre-applypatch.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/pre-push.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._pre-push.sample  \n",
            "  inflating: GraphEmbedding/.git/hooks/update.sample  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/hooks/._update.sample  \n",
            "   creating: GraphEmbedding/.git/refs/heads/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/refs/._heads  \n",
            "   creating: GraphEmbedding/.git/refs/remotes/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/refs/._remotes  \n",
            "  inflating: GraphEmbedding/data/flight/labels-usa-airports.txt  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/flight/._labels-usa-airports.txt  \n",
            "  inflating: GraphEmbedding/data/flight/brazil-airports.edgelist  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/flight/._brazil-airports.edgelist  \n",
            "  inflating: GraphEmbedding/data/flight/usa-airports.edgelist  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/flight/._usa-airports.edgelist  \n",
            "  inflating: GraphEmbedding/data/flight/labels-europe-airports.txt  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/flight/._labels-europe-airports.txt  \n",
            "  inflating: GraphEmbedding/data/flight/labels-brazil-airports.txt  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/flight/._labels-brazil-airports.txt  \n",
            "  inflating: GraphEmbedding/data/flight/europe-airports.edgelist  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/flight/._europe-airports.edgelist  \n",
            "  inflating: GraphEmbedding/data/wiki/Wiki_category.txt  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/wiki/._Wiki_category.txt  \n",
            "  inflating: GraphEmbedding/data/wiki/wiki_labels.txt  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/wiki/._wiki_labels.txt  \n",
            "  inflating: GraphEmbedding/data/wiki/Wiki_edgelist.txt  \n",
            "  inflating: __MACOSX/GraphEmbedding/data/wiki/._Wiki_edgelist.txt  \n",
            "  inflating: GraphEmbedding/.git/objects/pack/pack-b8f242fb1fb715177e08fca80e3e201abe177ec1.idx  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/objects/pack/._pack-b8f242fb1fb715177e08fca80e3e201abe177ec1.idx  \n",
            "  inflating: GraphEmbedding/.git/objects/pack/pack-b8f242fb1fb715177e08fca80e3e201abe177ec1.pack  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/objects/pack/._pack-b8f242fb1fb715177e08fca80e3e201abe177ec1.pack  \n",
            "   creating: GraphEmbedding/.git/logs/refs/heads/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/refs/._heads  \n",
            "   creating: GraphEmbedding/.git/logs/refs/remotes/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/refs/._remotes  \n",
            "  inflating: GraphEmbedding/.git/refs/heads/master  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/refs/heads/._master  \n",
            "   creating: GraphEmbedding/.git/refs/remotes/origin/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/refs/remotes/._origin  \n",
            "  inflating: GraphEmbedding/.git/logs/refs/heads/master  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/refs/heads/._master  \n",
            "   creating: GraphEmbedding/.git/logs/refs/remotes/origin/\n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/refs/remotes/._origin  \n",
            "  inflating: GraphEmbedding/.git/refs/remotes/origin/HEAD  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/refs/remotes/origin/._HEAD  \n",
            "  inflating: GraphEmbedding/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: __MACOSX/GraphEmbedding/.git/logs/refs/remotes/origin/._HEAD  \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=174k2qDmDhXrKFivGD00jBWAvJp9b1kiq\n",
            "To: /content/HARP.zip\n",
            "13.6MB [00:00, 20.5MB/s]\n",
            "Archive:  HARP.zip\n",
            "   creating: HARP/\n",
            "  inflating: HARP/default.walks.12   \n",
            "   creating: HARP/example_graphs/\n",
            "  inflating: HARP/default.walks.13   \n",
            "  inflating: HARP/Facebook.edges     \n",
            "  inflating: HARP/.DS_Store          \n",
            "  inflating: __MACOSX/HARP/._.DS_Store  \n",
            "  inflating: HARP/default.walks.9    \n",
            "  inflating: HARP/default.walks.0    \n",
            "  inflating: HARP/default.walks.7    \n",
            "  inflating: HARP/LICENSE            \n",
            "  inflating: HARP/requirements.txt   \n",
            "   creating: HARP/bin/\n",
            "  inflating: HARP/default.walks.6    \n",
            "  inflating: HARP/default.walks.1    \n",
            "   creating: HARP/dist/\n",
            "  inflating: HARP/default.walks.8    \n",
            "   creating: HARP/magicgraph.egg-info/\n",
            "  inflating: HARP/Facebook_emd.npy   \n",
            "  inflating: HARP/default.walks.11   \n",
            "  inflating: HARP/README.md          \n",
            "  inflating: HARP/default.walks.10   \n",
            "   creating: HARP/magicgraph/\n",
            "  inflating: HARP/.gitignore         \n",
            "  inflating: HARP/default.walks.4    \n",
            "  inflating: HARP/default.walks.3    \n",
            "  inflating: HARP/default.walks.2    \n",
            "  inflating: HARP/default.walks.5    \n",
            "   creating: HARP/build/\n",
            "   creating: HARP/.git/\n",
            "  inflating: HARP/contributors.txt   \n",
            "  inflating: HARP/Facebook_harp_node2vec_emd.npy  \n",
            "   creating: HARP/src/\n",
            "   creating: HARP/example_graphs/dblp/\n",
            "   creating: HARP/example_graphs/citeseer/\n",
            "   creating: HARP/example_graphs/blogcatalog/\n",
            "  inflating: HARP/bin/sfdp_windows.exe  \n",
            "  inflating: HARP/bin/sfdp_linux     \n",
            "  inflating: HARP/bin/sfdp_osx       \n",
            "  inflating: HARP/dist/magicgraph-0.0.1-py3.8.egg  \n",
            "  inflating: HARP/magicgraph.egg-info/PKG-INFO  \n",
            "  inflating: HARP/magicgraph.egg-info/not-zip-safe  \n",
            "  inflating: HARP/magicgraph.egg-info/SOURCES.txt  \n",
            "  inflating: HARP/magicgraph.egg-info/entry_points.txt  \n",
            "  inflating: HARP/magicgraph.egg-info/top_level.txt  \n",
            "  inflating: HARP/magicgraph.egg-info/dependency_links.txt  \n",
            "  inflating: HARP/magicgraph/LICENSE  \n",
            "  inflating: HARP/magicgraph/CONTRIBUTING.rst  \n",
            "   creating: HARP/magicgraph/dist/\n",
            "   creating: HARP/magicgraph/magicgraph.egg-info/\n",
            "   creating: HARP/magicgraph/tests/\n",
            "  inflating: HARP/magicgraph/MANIFEST.in  \n",
            "   creating: HARP/magicgraph/docs/\n",
            "  inflating: HARP/magicgraph/appveyor.yml  \n",
            "  inflating: HARP/magicgraph/setup.py  \n",
            "  inflating: HARP/magicgraph/.gitignore  \n",
            "  inflating: HARP/magicgraph/tox.ini  \n",
            "  inflating: HARP/magicgraph/AUTHORS.rst  \n",
            "  inflating: HARP/magicgraph/setup.cfg  \n",
            "   creating: HARP/magicgraph/build/\n",
            "  inflating: HARP/magicgraph/README.rst  \n",
            "  inflating: HARP/magicgraph/CHANGELOG.rst  \n",
            "   creating: HARP/magicgraph/.git/\n",
            "   creating: HARP/magicgraph/src/\n",
            "   creating: HARP/build/bdist.macosx-10.9-x86_64/\n",
            "  inflating: HARP/.git/config        \n",
            "   creating: HARP/.git/objects/\n",
            "  inflating: HARP/.git/HEAD          \n",
            "   creating: HARP/.git/info/\n",
            "   creating: HARP/.git/logs/\n",
            "  inflating: HARP/.git/description   \n",
            "   creating: HARP/.git/hooks/\n",
            "   creating: HARP/.git/refs/\n",
            "  inflating: HARP/.git/index         \n",
            "  inflating: HARP/.git/packed-refs   \n",
            "  inflating: HARP/.git/FETCH_HEAD    \n",
            "  inflating: HARP/src/graph_coarsening.py  \n",
            "  inflating: HARP/src/baseline.py    \n",
            "  inflating: HARP/src/scoring.py     \n",
            "  inflating: HARP/src/skipgram.pyc   \n",
            "  inflating: HARP/src/graph_coarsening.pyc  \n",
            "  inflating: HARP/src/baseline.pyc   \n",
            "  inflating: HARP/src/harp.py        \n",
            "  inflating: __MACOSX/HARP/src/._harp.py  \n",
            "  inflating: HARP/src/skipgram.py    \n",
            "  inflating: HARP/src/utils.py       \n",
            "  inflating: HARP/src/utils.pyc      \n",
            "  inflating: HARP/example_graphs/dblp/dblp.mat  \n",
            "  inflating: HARP/example_graphs/citeseer/citeseer.mat  \n",
            "  inflating: HARP/example_graphs/blogcatalog/blogcatalog.mat  \n",
            "  inflating: HARP/magicgraph/dist/magicgraph-0.0.1-py2.7.egg  \n",
            "  inflating: HARP/magicgraph/dist/magicgraph-0.0.1-py3.8.egg  \n",
            "  inflating: HARP/magicgraph/magicgraph.egg-info/PKG-INFO  \n",
            "  inflating: HARP/magicgraph/magicgraph.egg-info/not-zip-safe  \n",
            "  inflating: HARP/magicgraph/magicgraph.egg-info/SOURCES.txt  \n",
            "  inflating: HARP/magicgraph/magicgraph.egg-info/entry_points.txt  \n",
            "  inflating: HARP/magicgraph/magicgraph.egg-info/top_level.txt  \n",
            "  inflating: HARP/magicgraph/magicgraph.egg-info/dependency_links.txt  \n",
            "  inflating: HARP/magicgraph/tests/test_magicgraph.py  \n",
            "  inflating: HARP/magicgraph/docs/index.rst  \n",
            "  inflating: HARP/magicgraph/docs/requirements.txt  \n",
            "  inflating: HARP/magicgraph/docs/contributing.rst  \n",
            "  inflating: HARP/magicgraph/docs/conf.py  \n",
            "  inflating: HARP/magicgraph/docs/spelling_wordlist.txt  \n",
            "  inflating: HARP/magicgraph/docs/usage.rst  \n",
            "  inflating: HARP/magicgraph/docs/installation.rst  \n",
            "  inflating: HARP/magicgraph/docs/authors.rst  \n",
            "  inflating: HARP/magicgraph/docs/readme.rst  \n",
            "  inflating: HARP/magicgraph/docs/changelog.rst  \n",
            "   creating: HARP/magicgraph/docs/reference/\n",
            "   creating: HARP/magicgraph/build/bdist.macosx-10.9-x86_64/\n",
            "   creating: HARP/magicgraph/build/lib/\n",
            "   creating: HARP/magicgraph/build/bdist.macosx-11.4-x86_64/\n",
            "  inflating: HARP/magicgraph/.git/config  \n",
            "   creating: HARP/magicgraph/.git/objects/\n",
            "  inflating: HARP/magicgraph/.git/HEAD  \n",
            "   creating: HARP/magicgraph/.git/info/\n",
            "   creating: HARP/magicgraph/.git/logs/\n",
            "  inflating: HARP/magicgraph/.git/description  \n",
            "   creating: HARP/magicgraph/.git/hooks/\n",
            "   creating: HARP/magicgraph/.git/refs/\n",
            "  inflating: HARP/magicgraph/.git/index  \n",
            "  inflating: HARP/magicgraph/.git/packed-refs  \n",
            "   creating: HARP/magicgraph/src/magicgraph/\n",
            "   creating: HARP/.git/objects/0d/\n",
            "   creating: HARP/.git/objects/92/\n",
            "   creating: HARP/.git/objects/57/\n",
            "   creating: HARP/.git/objects/3b/\n",
            "   creating: HARP/.git/objects/04/\n",
            "   creating: HARP/.git/objects/35/\n",
            "   creating: HARP/.git/objects/67/\n",
            "   creating: HARP/.git/objects/93/\n",
            "   creating: HARP/.git/objects/0e/\n",
            "   creating: HARP/.git/objects/33/\n",
            "   creating: HARP/.git/objects/05/\n",
            "   creating: HARP/.git/objects/9d/\n",
            "   creating: HARP/.git/objects/9c/\n",
            "   creating: HARP/.git/objects/b3/\n",
            "   creating: HARP/.git/objects/da/\n",
            "   creating: HARP/.git/objects/b4/\n",
            "   creating: HARP/.git/objects/d1/\n",
            "   creating: HARP/.git/objects/d8/\n",
            "   creating: HARP/.git/objects/ab/\n",
            "   creating: HARP/.git/objects/e2/\n",
            "   creating: HARP/.git/objects/f4/\n",
            "   creating: HARP/.git/objects/c0/\n",
            "   creating: HARP/.git/objects/ee/\n",
            "   creating: HARP/.git/objects/fc/\n",
            "   creating: HARP/.git/objects/cf/\n",
            "   creating: HARP/.git/objects/e4/\n",
            "   creating: HARP/.git/objects/27/\n",
            "   creating: HARP/.git/objects/pack/\n",
            "   creating: HARP/.git/objects/29/\n",
            "   creating: HARP/.git/objects/87/\n",
            "   creating: HARP/.git/objects/80/\n",
            "   creating: HARP/.git/objects/7b/\n",
            "   creating: HARP/.git/objects/8f/\n",
            "   creating: HARP/.git/objects/7e/\n",
            "   creating: HARP/.git/objects/72/\n",
            "   creating: HARP/.git/objects/44/\n",
            "   creating: HARP/.git/objects/09/\n",
            "   creating: HARP/.git/objects/5d/\n",
            "   creating: HARP/.git/objects/info/\n",
            "   creating: HARP/.git/objects/91/\n",
            "   creating: HARP/.git/objects/65/\n",
            "   creating: HARP/.git/objects/54/\n",
            "   creating: HARP/.git/objects/3f/\n",
            "   creating: HARP/.git/objects/30/\n",
            "   creating: HARP/.git/objects/5b/\n",
            "   creating: HARP/.git/objects/52/\n",
            "   creating: HARP/.git/objects/0f/\n",
            "   creating: HARP/.git/objects/d4/\n",
            "   creating: HARP/.git/objects/a0/\n",
            "   creating: HARP/.git/objects/b1/\n",
            "   creating: HARP/.git/objects/aa/\n",
            "   creating: HARP/.git/objects/b7/\n",
            "   creating: HARP/.git/objects/db/\n",
            "   creating: HARP/.git/objects/a1/\n",
            "   creating: HARP/.git/objects/c4/\n",
            "   creating: HARP/.git/objects/e1/\n",
            "   creating: HARP/.git/objects/cd/\n",
            "   creating: HARP/.git/objects/f9/\n",
            "   creating: HARP/.git/objects/e9/\n",
            "   creating: HARP/.git/objects/e7/\n",
            "   creating: HARP/.git/objects/2c/\n",
            "   creating: HARP/.git/objects/77/\n",
            "   creating: HARP/.git/objects/84/\n",
            "   creating: HARP/.git/objects/24/\n",
            "   creating: HARP/.git/objects/4f/\n",
            "   creating: HARP/.git/objects/12/\n",
            "   creating: HARP/.git/objects/8c/\n",
            "   creating: HARP/.git/objects/71/\n",
            "   creating: HARP/.git/objects/47/\n",
            "   creating: HARP/.git/objects/78/\n",
            "   creating: HARP/.git/objects/13/\n",
            "   creating: HARP/.git/objects/7a/\n",
            "  inflating: HARP/.git/info/exclude  \n",
            "  inflating: HARP/.git/logs/HEAD     \n",
            "   creating: HARP/.git/logs/refs/\n",
            "  inflating: HARP/.git/hooks/commit-msg.sample  \n",
            "  inflating: HARP/.git/hooks/pre-rebase.sample  \n",
            "  inflating: HARP/.git/hooks/pre-commit.sample  \n",
            "  inflating: HARP/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: HARP/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: HARP/.git/hooks/pre-receive.sample  \n",
            "  inflating: HARP/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: HARP/.git/hooks/post-update.sample  \n",
            "  inflating: HARP/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: HARP/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: HARP/.git/hooks/pre-push.sample  \n",
            "  inflating: HARP/.git/hooks/update.sample  \n",
            "   creating: HARP/.git/refs/heads/\n",
            "   creating: HARP/.git/refs/tags/\n",
            "   creating: HARP/.git/refs/remotes/\n",
            "  inflating: HARP/magicgraph/docs/reference/index.rst  \n",
            "  inflating: HARP/magicgraph/docs/reference/magicgraph.rst  \n",
            "   creating: HARP/magicgraph/build/lib/magicgraph/\n",
            "   creating: HARP/magicgraph/.git/objects/pack/\n",
            "   creating: HARP/magicgraph/.git/objects/info/\n",
            "  inflating: HARP/magicgraph/.git/info/exclude  \n",
            "  inflating: HARP/magicgraph/.git/logs/HEAD  \n",
            "   creating: HARP/magicgraph/.git/logs/refs/\n",
            "  inflating: HARP/magicgraph/.git/hooks/commit-msg.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/pre-rebase.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/pre-commit.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/pre-receive.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/post-update.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/pre-merge-commit.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/pre-push.sample  \n",
            "  inflating: HARP/magicgraph/.git/hooks/update.sample  \n",
            "   creating: HARP/magicgraph/.git/refs/heads/\n",
            "   creating: HARP/magicgraph/.git/refs/tags/\n",
            "   creating: HARP/magicgraph/.git/refs/remotes/\n",
            "  inflating: HARP/magicgraph/src/magicgraph/__init__.py  \n",
            "  inflating: HARP/magicgraph/src/magicgraph/visualization.py  \n",
            "  inflating: HARP/magicgraph/src/magicgraph/generators.py  \n",
            "  inflating: HARP/magicgraph/src/magicgraph/__main__.py  \n",
            "  inflating: HARP/.git/objects/0d/20b6487c61e7d1bde93acf4a14b7a89083a16d  \n",
            "  inflating: HARP/.git/objects/92/8ab39a955fc9ffa22cfb319cda44087427d573  \n",
            "  inflating: HARP/.git/objects/57/221785fb742b6ac034a7ba1202e7cd1e79e59e  \n",
            "  inflating: HARP/.git/objects/3b/e0f32bcb9310b70e35201f32a1f1966b5ea6f6  \n",
            "  inflating: HARP/.git/objects/04/aa03807ac2fe563c47966216e9b81b99d0c3de  \n",
            "  inflating: HARP/.git/objects/35/e3ae4165d0e1944b08e9c9fb52beb636994817  \n",
            "  inflating: HARP/.git/objects/35/065f8ec6be4dd6480e577fd0923bb195ba01c7  \n",
            "  inflating: HARP/.git/objects/67/edf26f7470cc42bede6d79d16b659216fc27a4  \n",
            "  inflating: HARP/.git/objects/93/b978492b6396b355f4dd6d70beea0ed0f5174a  \n",
            "  inflating: HARP/.git/objects/93/f6165266a441c141303a7b75216d3e0f603b18  \n",
            "  inflating: HARP/.git/objects/0e/70e461c3a2c767425bb0645dfa2d9c3cc14951  \n",
            "  inflating: HARP/.git/objects/33/b45cad52c5bb20f67514cd6636268eec3a381e  \n",
            "  inflating: HARP/.git/objects/05/d0ffd438888d6b14cac4066395adc89314f9b9  \n",
            "  inflating: HARP/.git/objects/9d/792a4e3dc6bda94952a120c073b8df65d1702f  \n",
            "  inflating: HARP/.git/objects/9c/d355e51f3cd45587df8fccf15fea0f55f635b6  \n",
            "  inflating: HARP/.git/objects/b3/d0b7e69b083e471d7c0f9023042ebd32ce00a5  \n",
            "  inflating: HARP/.git/objects/b3/478af75dcbe5e5c20f3d2b305524548a1c473f  \n",
            "  inflating: HARP/.git/objects/da/7d8a65af0cf0f7f62872f4b3b39293ee74e97d  \n",
            "  inflating: HARP/.git/objects/b4/8d749d9c8ff1dd4c1b3e258afc4cccab659971  \n",
            "  inflating: HARP/.git/objects/d1/62215dd30bd058ab84229670c3224bb4c0d295  \n",
            "  inflating: HARP/.git/objects/d1/013063e970c8761eae22f295dbd531a9baa826  \n",
            "  inflating: HARP/.git/objects/d8/a2052b8fb280e0305546a4181f09ecf6974ac1  \n",
            "  inflating: HARP/.git/objects/d8/249d608d01fa2628c01a5c25603ef7f8c50222  \n",
            "  inflating: HARP/.git/objects/ab/d4f16ce101c139ab0fb8d39546f7e97960721f  \n",
            "  inflating: HARP/.git/objects/e2/6589542b85c12223bce0ccaa97b26aa29143ab  \n",
            "  inflating: HARP/.git/objects/f4/35d82d63f160587763f62c3902fe77b1671c3b  \n",
            "  inflating: HARP/.git/objects/c0/9ef641653683380003c81c84c7076245bcd8d4  \n",
            "  inflating: HARP/.git/objects/ee/a3991ed0df82902dd0b8166cdc2af0e4274c36  \n",
            "  inflating: HARP/.git/objects/fc/2f823ae1328b1ba614b164dccf6eca99d03be9  \n",
            "  inflating: HARP/.git/objects/cf/477b33c68fb26f99e18c0722788d0de6f55c54  \n",
            "  inflating: HARP/.git/objects/e4/a476e650bd2443fbdfebcf37235060528165da  \n",
            "  inflating: HARP/.git/objects/27/d84072c714c03e6f0c2210dc89d8fffc4ef6e5  \n",
            "  inflating: HARP/.git/objects/29/4920531ede02a48d8edd7f35ad605c724dfaca  \n",
            "  inflating: HARP/.git/objects/87/852cc89cfe3547f5f2af5e14ce7344349255f8  \n",
            "  inflating: HARP/.git/objects/80/98d2426aa4f71ba335b407486048c793e6ec5f  \n",
            "  inflating: HARP/.git/objects/80/be5f7c07d6bb09a8b7644573e1240244d6d513  \n",
            "  inflating: HARP/.git/objects/7b/515d1a908cf27c9787b7963dd5414aaf77a5fa  \n",
            "  inflating: HARP/.git/objects/8f/fdb0cbdce70b341a777097de088603bc8a4a9e  \n",
            "  inflating: HARP/.git/objects/7e/9eb147a3a9c03a73455fec22caf84395835557  \n",
            "  inflating: HARP/.git/objects/7e/db64c557d628255a671e34b512ca39e6e58524  \n",
            "  inflating: HARP/.git/objects/72/a13bf05ca58095f72574bb7adfc2718ae4d029  \n",
            "  inflating: HARP/.git/objects/44/b6fbdb1da406aa16c1cc77613f1acc5729f194  \n",
            "  inflating: HARP/.git/objects/09/2e9daf3c6d189f03565955ffbf2e00ad7656b7  \n",
            "  inflating: HARP/.git/objects/5d/d49d1c1753c4404b3d2ff5468afc8f1ddedffe  \n",
            "  inflating: HARP/.git/objects/91/ea425307d01ea8a5513317bdaa88947b8a9e6b  \n",
            "  inflating: HARP/.git/objects/65/089ca3364f36f541a2a73bddb5d755699325fd  \n",
            "  inflating: HARP/.git/objects/54/010b22bdd0411fe3249a931ac6e9fbe2c01c28  \n",
            "  inflating: HARP/.git/objects/3f/ee24e171fa8ad80d246c3804dd0169e72c7739  \n",
            "  inflating: HARP/.git/objects/30/bc25574a748765185b73a364fafbb52b6035aa  \n",
            "  inflating: HARP/.git/objects/5b/63e59ffb04e3c5c7204fb28fe20850cd7bbd73  \n",
            "  inflating: HARP/.git/objects/5b/8b4e482ad190b1aad0db760c6c93f75376a0d7  \n",
            "  inflating: HARP/.git/objects/52/799c7b989296e9ec3e1caa54e96909f82bd768  \n",
            "  inflating: HARP/.git/objects/0f/1758d9f65db25e8ba129110cabd01f21e5c8a1  \n",
            "  inflating: HARP/.git/objects/0f/66b00330cb22613644ee6e4170b3870e990286  \n",
            "  inflating: HARP/.git/objects/d4/c7a91e158b3018347feb9473453c3e1b734360  \n",
            "  inflating: HARP/.git/objects/a0/2cc577357b29eb6a57b69901ad027b10734549  \n",
            "  inflating: HARP/.git/objects/b1/45837e9667bf0baa6f3cd7d5321918686a8369  \n",
            "  inflating: HARP/.git/objects/aa/0384c55f271738a91877015bc86f173d9c66b0  \n",
            "  inflating: HARP/.git/objects/b7/67ef9324c95cbcff92e65a79bf27edfbc1366f  \n",
            "  inflating: HARP/.git/objects/db/c7ad649a3b375502fe738c80473e8af3b7b45b  \n",
            "  inflating: HARP/.git/objects/a1/c28a5885412a4de85a0e8514cfccc088a5a8ec  \n",
            "  inflating: HARP/.git/objects/c4/ce8080f57881481a27f87cd33386c32c473727  \n",
            "  inflating: HARP/.git/objects/e1/c5c9497dc40ce6ef85a61e747b6a3eb7c45767  \n",
            "  inflating: HARP/.git/objects/cd/3a2a145e19432f0d5bc1f064976778e63c95b0  \n",
            "  inflating: HARP/.git/objects/f9/59b44c7458dd86aba1aeb5a14bf611172c6875  \n",
            "  inflating: HARP/.git/objects/e9/45e6b9986fe8d49cfe7c209088862789e80540  \n",
            "  inflating: HARP/.git/objects/e7/d77cbc5249740b5f9abeae3d052a7ce9ef2754  \n",
            "  inflating: HARP/.git/objects/2c/fcb46f84d345b7243296ae4ddad646be0e3383  \n",
            "  inflating: HARP/.git/objects/77/fc0794810096cb26ebd3093f2500cb3aa72c14  \n",
            "  inflating: HARP/.git/objects/77/f562b3d9d19b09f2a029f7aba5fa52edcd812e  \n",
            "  inflating: HARP/.git/objects/84/900b824e75f7b94038d076bbaefc258873198b  \n",
            "  inflating: HARP/.git/objects/24/ac5e6ce3ee6d09cbcd37e2c7e2836d6a23f7c7  \n",
            "  inflating: HARP/.git/objects/4f/75987c29c2bd0d82468b354e94bc6faccd4fba  \n",
            "  inflating: HARP/.git/objects/12/ce48d7fec6c1d0f2b7448060a8356959f90db4  \n",
            "  inflating: HARP/.git/objects/8c/26effc881a64f43f97998548f46afaf3b9e030  \n",
            "  inflating: HARP/.git/objects/71/d949864f31d5ebdb6c57870c38c313687475c7  \n",
            "  inflating: HARP/.git/objects/71/e3a97d23f64fecb23b7f58a614151ba15630c0  \n",
            "  inflating: HARP/.git/objects/47/14d18c9cec40bbf0d3b3b1d0db8a16aecefba0  \n",
            "  inflating: HARP/.git/objects/78/b6ce11fc66bb984bd67d36ca0c765c1a0b813e  \n",
            "  inflating: HARP/.git/objects/13/9f7d430b673463ab28cf1560f9bd46fd828d76  \n",
            "  inflating: HARP/.git/objects/7a/fd5bc0c7d57027ad1b3a4a868f00c2d9a00e8d  \n",
            "   creating: HARP/.git/logs/refs/heads/\n",
            "   creating: HARP/.git/logs/refs/remotes/\n",
            "  inflating: HARP/.git/refs/heads/master  \n",
            "   creating: HARP/.git/refs/remotes/origin/\n",
            "  inflating: HARP/magicgraph/build/lib/magicgraph/__init__.py  \n",
            "  inflating: HARP/magicgraph/build/lib/magicgraph/visualization.py  \n",
            "  inflating: HARP/magicgraph/build/lib/magicgraph/generators.py  \n",
            "  inflating: HARP/magicgraph/build/lib/magicgraph/__main__.py  \n",
            "  inflating: HARP/magicgraph/.git/objects/pack/pack-da5d8629c32549826dbc3db3843f723826cc2b56.pack  \n",
            "  inflating: HARP/magicgraph/.git/objects/pack/pack-da5d8629c32549826dbc3db3843f723826cc2b56.idx  \n",
            "   creating: HARP/magicgraph/.git/logs/refs/heads/\n",
            "   creating: HARP/magicgraph/.git/logs/refs/remotes/\n",
            "  inflating: HARP/magicgraph/.git/refs/heads/master  \n",
            "   creating: HARP/magicgraph/.git/refs/remotes/origin/\n",
            "  inflating: HARP/.git/logs/refs/heads/master  \n",
            "   creating: HARP/.git/logs/refs/remotes/origin/\n",
            "  inflating: HARP/.git/refs/remotes/origin/HEAD  \n",
            "  inflating: HARP/magicgraph/.git/logs/refs/heads/master  \n",
            "   creating: HARP/magicgraph/.git/logs/refs/remotes/origin/\n",
            "  inflating: HARP/magicgraph/.git/refs/remotes/origin/HEAD  \n",
            "  inflating: HARP/.git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: HARP/magicgraph/.git/logs/refs/remotes/origin/HEAD  \n",
            "Collecting node2vec\n",
            "  Downloading node2vec-0.4.3.tar.gz (4.6 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from node2vec) (2.5.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from node2vec) (3.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from node2vec) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from node2vec) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.7/dist-packages (from node2vec) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->node2vec) (5.1.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->node2vec) (4.4.2)\n",
            "Building wheels for collected packages: node2vec\n",
            "  Building wheel for node2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for node2vec: filename=node2vec-0.4.3-py3-none-any.whl size=5980 sha256=d9e894f5663cf108edf223fdf6823e8d986acafa3084388f44bf6a92e2e0ed04\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/62/78/5202cb8c03cbf1593b48a8a442fca8ceec2a8c80e22318bae9\n",
            "Successfully built node2vec\n",
            "Installing collected packages: node2vec\n",
            "Successfully installed node2vec-0.4.3\n",
            "Collecting deepwalk\n",
            "  Downloading deepwalk-1.0.3-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Cython>=0.20.2 in /usr/local/lib/python3.7/dist-packages (from deepwalk) (0.29.23)\n",
            "Collecting argparse>=1.2.1\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: six>=1.7.3 in /usr/local/lib/python3.7/dist-packages (from deepwalk) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from deepwalk) (0.36.2)\n",
            "Requirement already satisfied: psutil>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from deepwalk) (5.4.8)\n",
            "Collecting futures>=2.1.6\n",
            "  Downloading futures-3.1.1-py3-none-any.whl (2.8 kB)\n",
            "Requirement already satisfied: gensim>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from deepwalk) (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from deepwalk) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=1.0.0->deepwalk) (5.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim>=1.0.0->deepwalk) (1.19.5)\n",
            "Installing collected packages: futures, argparse, deepwalk\n",
            "Successfully installed argparse-1.4.0 deepwalk-1.0.3 futures-3.1.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N424bAbUZ1GF"
      },
      "source": [
        "## Set Flags and Seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AS27mlESXLVW"
      },
      "source": [
        "dataset = \"facebook\" # possible values: facebook, douban, youtube\n",
        "is_new_data_split = True # sets whether to create new data and train set or rely on existing files which were downloaded before\n",
        "landmark_technique = \"random\" # possible values: random, coarsening, community_detection\n",
        "embedding_method = \"node2vec\" # possible values: node2vec, struc2vec, deepwalk, line, sdne, harp_node2vec, harp_deepwalk, harp_line\n",
        "\n",
        "# Setting different seeds for reproducability\n",
        "seed_value= 122\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "tf.random.set_seed(seed_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zarxfDHgZ4ic"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QrnA3I5aEsK"
      },
      "source": [
        "### Create graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_FhLhpsWzb1",
        "outputId": "e52eaf97-b957-41e4-81ad-8dfa80a523bd"
      },
      "source": [
        "def read_data(dataset):\n",
        "  if dataset == \"facebook\":\n",
        "      G=nx.read_edgelist(\"./datasets/facebook_edges.txt\")\n",
        "  elif dataset == \"douban\":\n",
        "      G = nx.read_edgelist('./datasets/douban_edges.csv', delimiter=',', nodetype=int, encoding=\"utf-8\")\n",
        "      G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering=\"sorted\")\n",
        "      mapping = {}\n",
        "      for v in G.nodes():\n",
        "        mapping[v] = str(v)\n",
        "      G = nx.relabel.relabel_nodes(G, mapping)\n",
        "  elif dataset == \"youtube\":\n",
        "      G = nx.read_edgelist('./datasets/youtube_edges.csv', delimiter=',', nodetype=int, encoding=\"utf-8\")\n",
        "      G = nx.relabel.convert_node_labels_to_integers(G, first_label=0, ordering=\"sorted\")\n",
        "      mapping = {}\n",
        "      for v in G.nodes():\n",
        "        mapping[v] = str(v)\n",
        "      G = nx.relabel.relabel_nodes(G, mapping)\n",
        "\n",
        "  nodes = list(G.nodes())\n",
        "  edges = list(G.edges())\n",
        "  num_nodes = len(nodes)\n",
        "  num_edges = len(edges)\n",
        "  print(\"Number of nodes\", num_nodes)\n",
        "  print(\"Number of edges\", num_edges)\n",
        "  return G, nodes, edges, num_nodes, num_edges\n",
        "\n",
        "G, nodes, edges, num_nodes, num_edges = read_data(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of nodes 4039\n",
            "Number of edges 88234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M48aM88aLcS"
      },
      "source": [
        "### Split Train-Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyV9ByJB9QW_"
      },
      "source": [
        "#### Landmark Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z64-8b7tZZP_"
      },
      "source": [
        "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
        "from HARP.src.harp import run_coarsening, train_embedding\n",
        "\n",
        "def select_landmarks(num_nodes, landmark_technique,  nodes, is_new_data_split):\n",
        "  if is_new_data_split:\n",
        "    print(\"#### LANDMARK SELECTION ####\")\n",
        "\n",
        "    # Select number of landmark nodes\n",
        "    if (num_nodes>10000):\n",
        "      k1=3\n",
        "      k2=1\n",
        "    else:\n",
        "      k1=100\n",
        "      k2=11\n",
        "\n",
        "    # Select training landmarks\n",
        "    if landmark_technique == \"random\":\n",
        "      print(\"Selecting landmarks randomly...\")\n",
        "      k1_nodes = random.sample(nodes,k1)\n",
        "    elif landmark_technique == \"coarsening\":\n",
        "      print(\"Coarsening graph...\")\n",
        "      %cd HARP\n",
        "      recursive_node_assosiations=run_coarsening(\"../datasets/facebook_edges.txt\", None, \"edgelist\")[1]\n",
        "      #embeddings = train_embedding(\"line\", \"Facebook_HARP_line.npx\", \"../datasets/facebook_edges.txt\", \"network\")\n",
        "      %cd ..\n",
        "      for rec_level in recursive_node_assosiations[::-1]:\n",
        "        rec_l = list(rec_level.keys())\n",
        "        #print(len(rec_l))\n",
        "        if len(rec_l) >= k1:\n",
        "          print(\"Selecting landmarks as coarsened graph...\")\n",
        "          k1_nodes = random.sample(rec_l, k1)\n",
        "          k1_nodes = [str(node) for node in k1_nodes]\n",
        "\n",
        "          break\n",
        "    elif landmark_technique == \"community_detection\":\n",
        "      c = list(greedy_modularity_communities(G))\n",
        "      ratios = []\n",
        "      sum = 0\n",
        "      k1_nodes = []\n",
        "      print(\"Clustering into communities...\")\n",
        "      print(\"number of communities: \",c)\n",
        "      for i in range(len(c)):\n",
        "          percentage = int((len(c[i])/num_nodes)*100)\n",
        "          if percentage==0:\n",
        "              percentage = 1\n",
        "          k1_nodes.extend(random.sample(c[i],percentage))\n",
        "    print(\"Number of training landmark nodes:\",len(k1_nodes))\n",
        "    remaining_nodes_train = list(set(nodes)-set(k1_nodes))\n",
        "    print(\"number of nodes except training (landmark) nodes\", len(remaining_nodes_train))\n",
        "\n",
        "    # Select testing landmarks\n",
        "    k2_nodes = random.sample(remaining_nodes_train,k2)\n",
        "\n",
        "    print(\"Number of testing landmark nodes:\",len(k2_nodes))\n",
        "    remaining_nodes_test = list(set(nodes)-set(k2_nodes))\n",
        "    print(\"number of nodes except testing (landmark) nodes\", len(remaining_nodes_test))\n",
        "    return k1_nodes, k2_nodes, remaining_nodes_train, remaining_nodes_test\n",
        "  else:\n",
        "    pass\n",
        "\n",
        "#k1_nodes, k2_nodes, remaining_nodes_train, remaining_nodes_test = select_landmarks(num_nodes, \"coarsening\", nodes, is_new_data_split) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GbtDsH63yOY"
      },
      "source": [
        "#### Create train set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltcgTe6O0Nlr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "2757f481-cca3-4748-a6c4-7600ae32a779"
      },
      "source": [
        "# Create and save new train set\n",
        "def generate_and_save_train_data(G, k1_nodes, is_new_data_split):\n",
        "  train_set = []\n",
        "  if is_new_data_split:\n",
        "    for u in k1_nodes:\n",
        "      for v in remaining_nodes_train:\n",
        "          if nx.has_path(G, u, v):\n",
        "            shortest_path = nx.shortest_path(G, u, v)\n",
        "            length = 1\n",
        "            for i in range(len(shortest_path)-1):\n",
        "              train_set.append((shortest_path[0], shortest_path[i+1], length))\n",
        "              length +=1\n",
        "\n",
        "    print(\"Size of total training set before omission:\",len(train_set))\n",
        "\n",
        "    f_train = open('./datasets/train.txt', 'w')\n",
        "    for i in range(len(train_set)): \n",
        "      if (1< train_set[i][2] <= 6):\n",
        "        f_train.write(str(train_set[i][0])+' '+str(train_set[i][1])+' '+str(train_set[i][2]) )\n",
        "        f_train.write('\\n')\n",
        "              \n",
        "    f_train.close()\n",
        "    print(\"Train file written\")\n",
        "\n",
        "generate_and_save_train_data(G, k1_nodes, is_new_data_split)\n",
        "# Load train set from txt\n",
        "def load_training_data():\n",
        "  train_list1 = []\n",
        "  train_list2 = []\n",
        "  y_train = []\n",
        "  f_train = open(\"./datasets/train.txt\", 'r')\n",
        "  for line in f_train:\n",
        "    a=line.strip('\\n').split(' ')\n",
        "    train_list1.append(int(a[0])) \n",
        "    train_list2.append(int(a[1]))\n",
        "    y_train.append(int(a[2]))\n",
        "  f_train.close()\n",
        "  print(\"Number of training pairs:\", len(y_train))\n",
        "  print(\"Path Lengths in train set: \",np.unique(np.array(y_train)),\" Size of each length: \", np.unique(np.array(y_train),return_counts=True)[1])\n",
        "  return train_list1, train_list2, y_train\n",
        "#train_list1, train_list2, y_train = load_training_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-16872ef245ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train file written\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mgenerate_and_save_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk1_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_new_data_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# Load train set from txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'k1_nodes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSRj70Tc38VA"
      },
      "source": [
        "#### Create test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYEkT3W45rHP"
      },
      "source": [
        "test_set = []\n",
        "# Create and save new test split\n",
        "def generate_and_save_test_data(G, k2_nodes, is_new_data_split):\n",
        "  if is_new_data_split:\n",
        "    for u in k2_nodes:\n",
        "      for v in remaining_nodes_test:\n",
        "        if nx.has_path(G, u, v):\n",
        "          shortest_path = nx.shortest_path(G,u,v)\n",
        "          length = 1\n",
        "          for i in range(len(shortest_path) - 1):\n",
        "            test_set.append((shortest_path[0], shortest_path[i+1], length))\n",
        "            length += 1\n",
        "    print(\"Size of total training set before omission:\",len(test_set))\n",
        "\n",
        "    f_test = open(\"./datasets/test.txt\", 'w')\n",
        "    for i in range(len(test_set)):\n",
        "      if (1< test_set[i][2] <= 6):\n",
        "        f_test.write(str(test_set[i][0])+' '+ str(test_set[i][1]) +' '+ str(test_set[i][2]) )\n",
        "        f_test.write('\\n')\n",
        "\n",
        "    f_test.close()\n",
        "    print(\"Test file written\")\n",
        "#generate_and_save_test_data(G, k2_nodes, is_new_data_split)\n",
        "\n",
        "def load_testing_data():\n",
        "\n",
        "  y_test = []\n",
        "\n",
        "  # Load test set from txt\n",
        "  test_list1=[]\n",
        "  test_list2=[]    \n",
        "  f_test= open('./datasets/test.txt', 'r') \n",
        "\n",
        "  for line in f_test:\n",
        "    a=line.strip('\\n').split(' ')\n",
        "    test_list1.append(int(a[0])) \n",
        "    test_list2.append(int(a[1]))\n",
        "    y_test.append(int(a[2]))\n",
        "  f_test.close()\n",
        "  print(\"Number of testing pairs:\", len(y_test))\n",
        "  print(\"Path Lengths in test set: \",np.unique(np.array(y_test)),\" Size of each length: \", np.unique(np.array(y_test),return_counts=True)[1])\n",
        "  return test_list1, test_list2, y_test\n",
        "#test_list1, test_list2, y_test = load_testing_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPA_TGuVd0Hz"
      },
      "source": [
        "### Visualize graph with landmarks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiDJATXGcEGH"
      },
      "source": [
        "# Needs download from colab and reopen local\n",
        "def visualise_graph_with_landmarks(G, k1_nodes, landmark_technique):\n",
        "  net = Network(notebook=True)\n",
        "  net.from_nx(G)\n",
        "  # Coloring landmark nodes\n",
        "  for node_id in k1_nodes:\n",
        "    net.node_map[str(node_id)]['shape'] = 'box'\n",
        "    net.node_map[str(node_id)]['color'] = 'red'\n",
        "  net.save_graph('facebook_landmarks_128_'+landmark_technique+'.html')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm8RRfRWZsFS"
      },
      "source": [
        "## Create Emeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKqrzz5ekPu9"
      },
      "source": [
        "#### Train embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56myKS58kSOn"
      },
      "source": [
        "from GraphEmbedding.ge.models import Node2Vec, DeepWalk, SDNE, Struc2Vec, LINE\n",
        "from collections import OrderedDict\n",
        "\n",
        "def train_embedding(G, params={}, embedding_method=\"node2vec\"):\n",
        "  print(\"Params for Embeddings: \", params, \" with embedding type: \", embedding_method)\n",
        "\n",
        "  if embedding_method == \"node2vec\":\n",
        "    q=1\n",
        "    p=1\n",
        "    window_size = 5\n",
        "    if params:\n",
        "      if 'q' in params.keys():\n",
        "        q = params['q']\n",
        "      if 'p' in params.keys():\n",
        "        p = params['p']\n",
        "      if 'window_size' in params.keys():\n",
        "        window_size = params['window_size']\n",
        "    model = Node2Vec(G, 80, 10, q=q, p=p) # default params from paper implementation\n",
        "    model.train(window_size = window_size)\n",
        "    embedding_vectors = model.get_embeddings()\n",
        "  elif embedding_method == \"deepwalk\":\n",
        "    model = DeepWalk(G, 80, 40) # parameters as in the paper\n",
        "    model.train()\n",
        "    embedding_vectors = model.get_embeddings()\n",
        "  elif embedding_method == \"sdne\":\n",
        "    layer_config = [400, 128]\n",
        "    if params:\n",
        "      layer_config = params[\"layer_config\"]\n",
        "    model = SDNE(G, hidden_size=layer_config) # same hidden sizes as in paper for arxiv GR-QC as it has similar number of nodes as facebook\n",
        "    model.train( epochs=40, verbose=0)\n",
        "    embedding_vectors = model.get_embeddings()\n",
        "  elif embedding_method == \"struc2vec\":\n",
        "    model = Struc2Vec(G)\n",
        "    model.train()\n",
        "    embedding_vectors = model.get_embeddings()\n",
        "  elif embedding_method == \"line\":\n",
        "    order = \"second\"\n",
        "    if params:\n",
        "      order = params[\"order\"]\n",
        "    model = LINE(G, embedding_size= 128, order=order)\n",
        "    model.train(epochs=50, verbose=0)\n",
        "    embedding_vectors = model.get_embeddings()\n",
        "  elif embedding_method == \"harp_deepwalk\":\n",
        "    embedding_vectors_res = np.load(\"./datasets/Facebook_HARP_deepwalk.npy\")\n",
        "  elif embedding_method == \"harp_node2vec\":\n",
        "    embedding_vectors_res = np.load(\"./datasets/Facebook_HARP_node2vec.npy\")\n",
        "  elif embedding_method == \"harp_line\":\n",
        "    embedding_vectors_res = np.load(\"./datasets/Facebook_HARP_line.npy\")\n",
        "\n",
        "  if embedding_method not in [\"harp_deepwalk\", \"harp_node2vec\", \"harp_line\"]:\n",
        "    #embedding_vectors = OrderedDict(sorted(embedding_vectors.items()))\n",
        "    num_nodes = max([int(emb) for emb in embedding_vectors.keys()])\n",
        "\n",
        "    embedding_vectors_res = np.array(list(embedding_vectors.values()))\n",
        "    for i in range(num_nodes):\n",
        "      embedding_vectors_res[i] = embedding_vectors[str(i)]\n",
        "  return embedding_vectors_res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJcSNYx8HJIH"
      },
      "source": [
        "### Preprocess embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snhYV62ZvyP6"
      },
      "source": [
        "def preprocess_embeddings(embedding_vectors, train_list1, train_list2, test_list1, test_list2, emb_size = 128, con_type = \"avg\"):\n",
        "  X_train_binary = []\n",
        "  X_test_binary = []\n",
        "  for i in range(len(train_list1)):\n",
        "    if con_type == \"avg\":\n",
        "      con_x = (embedding_vectors[train_list1[i]] + embedding_vectors[train_list2[i]])/2\n",
        "    elif con_type == \"concat\":\n",
        "      con_x = np.concatenate( [embedding_vectors[train_list1[i]],embedding_vectors[train_list2[i]]], axis=0)\n",
        "    elif con_type == \"hadamard\":\n",
        "      con_x = embedding_vectors[train_list1[i]] * embedding_vectors[train_list2[i]]\n",
        "    elif con_type ==\"sub\":\n",
        "      con_x = embedding_vectors[train_list1[i]] - embedding_vectors[train_list2[i]]\n",
        "    elif con_type ==\"conv\":\n",
        "      con_x = [embedding_vectors[train_list1[i]],embedding_vectors[train_list2[i]]]\n",
        "\n",
        "\n",
        "    X_train_binary.append(con_x)\n",
        "\n",
        "  for i in range(len(test_list1)):\n",
        "    if con_type == \"avg\":\n",
        "      con_x = (embedding_vectors[test_list1[i]] + embedding_vectors[test_list2[i]])/2\n",
        "    elif con_type == \"concat\":\n",
        "      con_x = np.concatenate( [embedding_vectors[test_list1[i]],embedding_vectors[test_list2[i]]], axis=0)\n",
        "    elif con_type == \"hadamard\":\n",
        "      con_x = embedding_vectors[test_list1[i]] * embedding_vectors[test_list2[i]]\n",
        "    elif con_type ==\"sub\":\n",
        "      con_x = embedding_vectors[test_list1[i]] - embedding_vectors[test_list2[i]]\n",
        "    elif con_type ==\"conv\":\n",
        "      con_x = [embedding_vectors[test_list1[i]],embedding_vectors[test_list2[i]]]\n",
        "\n",
        "\n",
        "    X_test_binary.append(con_x)\n",
        "\n",
        "  print(\"Embedded training set size:\", len(X_train_binary))\n",
        "  print(\"Embedded test set size:\", len(X_test_binary))\n",
        "\n",
        "  train_test_split = [np.array(X_train_binary), np.array(y_train), np.array(X_test_binary), np.array(y_test)]\n",
        "  if con_type == \"conv\":\n",
        "    train_test_split[0] = train_test_split[0].reshape(-1, 2, emb_size, 1)\n",
        "    train_test_split[2] = train_test_split[2].reshape(-1, 2, emb_size, 1)\n",
        "\n",
        "  print(\"Train test split summary: \\n\", \"Train X shape:\", train_test_split[0].shape\\\n",
        "        ,\"Train y shape:\", train_test_split[1].shape,\n",
        "        \"\\n Test X shape:\", train_test_split[2].shape,\n",
        "        \"Test y shape:\", train_test_split[3].shape)\n",
        "  return train_test_split\n",
        "\n",
        "# Shuffle for train data before training\n",
        "def shuffle_in_unison(a, b):\n",
        "    rng_state = np.random.get_state()\n",
        "    np.random.shuffle(a)\n",
        "    np.random.set_state(rng_state)\n",
        "    np.random.shuffle(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK71AWg3hc9O"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmOThLt3ZE2b"
      },
      "source": [
        "## MLP Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17FvYGneG-74"
      },
      "source": [
        "import tensorflow.keras as keras \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,BatchNormalization,Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "def create_regression_model(input_dim, emb_size=128):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(emb_size, input_dim=input_dim))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  dense_size= int(0.2*input_dim)\n",
        "  model.add(Dense(dense_size))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('softplus'))\n",
        "\n",
        "  opt=SGD(learning_rate=0.05, momentum=0.2)\n",
        "  model.compile(loss='mse', optimizer='sgd', metrics=['mae'])\n",
        "  #model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVGE_gZrr9af"
      },
      "source": [
        "## MLP Regression with Conv Concat"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQRurdsBsBf1"
      },
      "source": [
        "def create_regression_conv_model(emb_size=128):\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(1, (2,1),kernel_constraint=\"non_neg\",use_bias=False,input_shape=(2,emb_size,1)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(emb_size))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  dense_size= int(0.2*emb_size)\n",
        "  model.add(Dense(dense_size))\n",
        "\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.4))\n",
        "\n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('softplus'))\n",
        "\n",
        "  opt=SGD(learning_rate=0.05, momentum=0.01)\n",
        "  model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf-Wz7QUGrVT"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BTIHFGkC5_"
      },
      "source": [
        "import keras.callbacks as cb\n",
        "from keras.callbacks import LambdaCallback\n",
        "\n",
        "def train_regressor(model, X_train, y_train, epochs=15, batch_size = 32):\n",
        "  class LossHistory(cb.Callback):\n",
        "      def on_train_begin(self, logs={}):\n",
        "          self.losses = []\n",
        "\n",
        "      def on_batch_end(self, batch, logs={}):\n",
        "          batch_loss = logs.get('loss')\n",
        "          self.losses.append(batch_loss)\n",
        "  history = LossHistory()\n",
        "  model.fit(X_train, y_train, epochs= epochs, batch_size= batch_size, callbacks=[history], validation_split= 0.3, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGpI_bBUG9Iw"
      },
      "source": [
        "# Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj94v-Fx-hT9"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def test_model(model,X_train, X_test, y_test):\n",
        "  preds = model.predict(X_test)\n",
        "  pred = []\n",
        "\n",
        "  for i in range(len(preds)):\n",
        "    pred.append(round(float(preds[i][0])))\n",
        "\n",
        "  rmse = (mean_squared_error(y_test, pred))**0.5\n",
        "  mae = mean_absolute_error(y_test, pred)\n",
        "  print(\"RMSE:\", rmse, \"MAE:\", mae, \"\\nSummary:\", \"\\nNodes:\", len(G.nodes()),\n",
        "        \"Edges:\", len(G.edges()), \"Train size:\", len(X_train), \"Test size:\", len(X_test))\n",
        "  \n",
        "  accuracy = accuracy_score(y_test,pred)\n",
        "  print('Accuracy: %f' % accuracy)\n",
        "  print('Paths: \\t\\t',np.unique(y_test))\n",
        "  precision = precision_score(y_test,pred,average=None)\n",
        "  print('Precision: \\t' , precision)\n",
        "  recall = recall_score(y_test,pred,average=None)\n",
        "  print('Recall: \\t' , recall)\n",
        "  f1 = f1_score(y_test,pred,average=None)\n",
        "  print('F1 score: \\t', f1)\n",
        "\n",
        "  return rmse, mae, accuracy, precision, recall, f1\n",
        "\n",
        "def test_model_detailed(model, X_test, y_test):\n",
        "  preds = model.predict(X_test)\n",
        "  pred = []\n",
        "\n",
        "  indexes_2 = []\n",
        "  indexes_3 = []\n",
        "  indexes_4 = []\n",
        "  indexes_5 = []\n",
        "  indexes_6 = []\n",
        "  y_test_2 = []\n",
        "  y_test_3 = []\n",
        "  y_test_4 = []\n",
        "  y_test_5 = []\n",
        "  y_test_6 = []\n",
        "\n",
        "  pred_2 = []\n",
        "  pred_3 = []\n",
        "  pred_4 = []\n",
        "  pred_5 = []\n",
        "  pred_6 = []\n",
        "\n",
        "  for i in range(len(y_test)):\n",
        "      if y_test[i]==2:\n",
        "          indexes_2.append(i)\n",
        "          y_test_2.append(y_test[i])\n",
        "      if y_test[i]==3:\n",
        "          indexes_3.append(i)\n",
        "          y_test_3.append(y_test[i])\n",
        "      if y_test[i]==4:\n",
        "          indexes_4.append(i)\n",
        "          y_test_4.append(y_test[i])\n",
        "      if y_test[i]==5:\n",
        "          indexes_5.append(i)\n",
        "          y_test_5.append(y_test[i])\n",
        "      if y_test[i]==6:\n",
        "          indexes_6.append(i)\n",
        "          y_test_6.append(y_test[i])\n",
        "\n",
        "  for i in range(len(y_test_2)):\n",
        "    pred_2.append(round(float(preds[indexes_2[i]][0])))\n",
        "\n",
        "  for i in range(len(y_test_3)):\n",
        "    pred_3.append(round(float(preds[indexes_3[i]][0])))\n",
        "\n",
        "  for i in range(len(y_test_4)):\n",
        "    pred_4.append(round(float(preds[indexes_4[i]][0])))\n",
        "\n",
        "  for i in range(len(y_test_5)):\n",
        "    pred_5.append(round(float(preds[indexes_5[i]][0])))\n",
        "\n",
        "  for i in range(len(y_test_6)):\n",
        "    pred_6.append(round(float(preds[indexes_6[i]][0])))\n",
        "\n",
        "  rmse2 = (mean_squared_error(y_test_2, pred_2))**0.5\n",
        "  mae2 = mean_absolute_error(y_test_2, pred_2)\n",
        "\n",
        "  rmse3 = (mean_squared_error(y_test_3, pred_3))**0.5\n",
        "  mae3 = mean_absolute_error(y_test_3, pred_3)\n",
        "\n",
        "  rmse4 = (mean_squared_error(y_test_4, pred_4))**0.5\n",
        "  mae4 = mean_absolute_error(y_test_4, pred_4)\n",
        "\n",
        "  rmse5 = (mean_squared_error(y_test_5, pred_5))**0.5\n",
        "  mae5 = mean_absolute_error(y_test_5, pred_5)\n",
        "\n",
        "  rmse6 = (mean_squared_error(y_test_6, pred_6))**0.5\n",
        "  mae6 = mean_absolute_error(y_test_6, pred_6)\n",
        "\n",
        "  print(\"RMSE at length 2:\", rmse2, \"MAE at length 2:\", mae2)\n",
        "  print(\"RMSE at length 3:\", rmse3, \"MAE at length 3:\", mae3)\n",
        "  print(\"RMSE at length 4:\", rmse4, \"MAE at length 4:\", mae4)\n",
        "  print(\"RMSE at length 5:\", rmse5, \"MAE at length 5:\", mae5)\n",
        "  print(\"RMSE at length 6:\", rmse6, \"MAE at length 6:\", mae6)\n",
        "  return rmse2, mae2, rmse3, mae3, rmse4, mae4, rmse5, mae5, rmse6, mae6\n",
        "\n",
        "def plot_test_losses(model, X_test, y_test):\n",
        "    rmse2, mae2, rmse3, mae3, rmse4, mae4, rmse5, mae5, rmse6, mae6 = test_model_detailed(model, X_test, y_test)\n",
        "\n",
        "    labels = ['SP2', 'SP3', 'SP4', 'SP5', 'SP6']\n",
        "    rmse_values = [rmse2, rmse3, rmse4, rmse5, rmse6]\n",
        "    mae_values = [mae2, mae3, mae4, mae5, mae6]\n",
        "\n",
        "    x = np.arange(len(labels))  # the label locations\n",
        "    width = 0.35  # the width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    rects1 = ax.bar(x - width/2, rmse_values, width, label='RMSE')\n",
        "    rects2 = ax.bar(x + width/2, mae_values, width, label='MAE')\n",
        "\n",
        "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "    ax.set_ylabel('Loss values')\n",
        "    ax.set_title('Losses by shortest path distance')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(labels)\n",
        "    ax.legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmZnGlX_JqBP"
      },
      "source": [
        "### Train test loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXqiKP_uu5uE"
      },
      "source": [
        "def train_test_loop_optim(emb_method, emb_size, epochs, emb_params):\n",
        "  embedding_vectors = train_embedding(G, embedding_method= emb_method, params = emb_params)\n",
        "\n",
        "  # Test with avg binary operator\n",
        "  print(\"Embedding method: \", emb_method, \"Binary operator: conv\")\n",
        "  X_train, y_train, X_test, y_test = preprocess_embeddings(embedding_vectors, train_list1, train_list2, test_list1, test_list2, emb_size,\"conv\")\n",
        "  shuffle_in_unison(X_train, y_train)\n",
        "  print(\"\\n\")\n",
        "  model = create_regression_conv_model(emb_size =128)\n",
        "  print(\"\\n\")\n",
        "  train_regressor(model, X_train, y_train, epochs=epochs)\n",
        "  print(\"\\n\")\n",
        "  test_model(model, X_train, X_test, y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0E1BdLxJslA"
      },
      "source": [
        "def train_test_loop(emb_method, emb_size, epochs, emb_params ):\n",
        "  print(\"#### TRAIN TEST LOOP\")\n",
        "  # Create Embeddings\n",
        "  embedding_vectors = train_embedding(G, embedding_method= emb_method, params = emb_params)\n",
        "\n",
        "  # Test with avg binary operator\n",
        "  print(\"Embedding method: \", emb_method, \"Binary operator: avg\")\n",
        "  X_train, y_train, X_test, y_test = preprocess_embeddings(embedding_vectors, train_list1, train_list2, test_list1, test_list2, emb_size,\"avg\")\n",
        "  shuffle_in_unison(X_train, y_train)\n",
        "  print(\"\\n\")\n",
        "  model = create_regression_model(X_train.shape[1])\n",
        "  print(\"\\n\")\n",
        "  train_regressor(model, X_train, y_train, epochs=epochs)\n",
        "  print(\"\\n\")\n",
        "  test_model(model, X_train, X_test, y_test)\n",
        "  plot_test_losses(model, X_test, y_test)\n",
        "  # Test with conc binary operator\n",
        "  print(\"Embedding method: \", emb_method, \"Binary operator: concat\")\n",
        "  X_train, y_train, X_test, y_test = preprocess_embeddings(embedding_vectors, train_list1, train_list2, test_list1, test_list2, emb_size,\"concat\")\n",
        "  shuffle_in_unison(X_train, y_train)\n",
        "  print(\"\\n\")\n",
        "  model = create_regression_model(X_train.shape[1])\n",
        "  print(\"\\n\")\n",
        "  train_regressor(model, X_train, y_train, epochs=epochs)\n",
        "  print(\"\\n\")\n",
        "  test_model(model, X_train, X_test, y_test)\n",
        "  plot_test_losses(model, X_test, y_test)\n",
        "  # Test with convolution\n",
        "  print(\"Embedding method: \", emb_method, \"Binary operator: conv\")\n",
        "  X_train, y_train, X_test, y_test = preprocess_embeddings(embedding_vectors, train_list1, train_list2, test_list1, test_list2, emb_size,\"conv\")\n",
        "  shuffle_in_unison(X_train, y_train)\n",
        "  print(\"\\n\")\n",
        "  model = create_regression_conv_model(emb_size =emb_size)\n",
        "  print(\"\\n\")\n",
        "  train_regressor(model, X_train, y_train, epochs=epochs)\n",
        "  print(\"\\n\")\n",
        "  test_model(model, X_train, X_test, y_test)\n",
        "  plot_test_losses(model, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXnXFErif3mw"
      },
      "source": [
        "### Running tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NObkwuzBwXqr"
      },
      "source": [
        "landmark_techniques = [\"random\", \"community_detection\", \"coarsening\"]\n",
        "embedding_techniques = [\"node2vec\", \"harp_node2vec\",\"sdne\", \"line\", \"harp_line\"]\n",
        "for landmark_technique in landmark_techniques:\n",
        "  is_new_data_split = landmark_technique != \"random\"\n",
        "  print(\"### LANDMARK RUN\", landmark_technique, \"###\")\n",
        "  G, nodes, edges, num_nodes, num_edges = read_data(dataset)\n",
        "  if is_new_data_split:\n",
        "    k1_nodes, k2_nodes, remaining_nodes_train, remaining_nodes_test = select_landmarks(num_nodes, landmark_technique, nodes, is_new_data_split)\n",
        "    visualise_graph_with_landmarks(G, k1_nodes, landmark_technique)\n",
        "    generate_and_save_train_data(G, k1_nodes, is_new_data_split)\n",
        "    generate_and_save_test_data(G, k2_nodes, is_new_data_split)\n",
        "\n",
        "  train_list1, train_list2, y_train = load_training_data()\n",
        "  test_list1, test_list2, y_test = load_testing_data()\n",
        "  \n",
        "  for embedding_technique in embedding_techniques:\n",
        "    print(\"### EMBEDDING RUN\", embedding_technique, \"###\")\n",
        "    train_test_loop(emb_method = embedding_technique, emb_size = 128, epochs= 15, emb_params = {}) # No embeddings params set as default values are the optimised ones\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FvsDnbHMbzW"
      },
      "source": [
        "### Embedding Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAfrx7X0Mfh7"
      },
      "source": [
        "def optimize_embeddings(): # requires to have a num_nodes variable before and also a train_test_loop method as defined above\n",
        "  embedding_types = [\"sdne\", \"line\", \"node2vec\"] #,\n",
        "  for embedding_type in embedding_types:\n",
        "    params = {}\n",
        "    if embedding_type == \"node2vec\":\n",
        "      ps = [0.1, 0.75, 1, 1.25, 2] \n",
        "      qs = [0.1, 0.75, 1, 1.25, 2]\n",
        "      for p in ps:\n",
        "        for q in qs:\n",
        "          params[\"p\"] = p\n",
        "          params[\"q\"] = q\n",
        "          train_test_loop_optim(emb_method=\"node2vec\", emb_size= 128, epochs = 15, emb_params = params)\n",
        "    elif embedding_type == \"line\":\n",
        "      first_or_second = [ \"first\", \"second\"]\n",
        "      for f_s in first_or_second:\n",
        "        params[\"order\"] = f_s\n",
        "        train_test_loop_optim(emb_method=\"line\", emb_size= 128, epochs = 15, emb_params = params)\n",
        "    elif embedding_type == \"sdne\":\n",
        "      layer_configs = [[int((num_nodes*0.1)), 128], [int((num_nodes*0.1)), int((num_nodes*0.05)), 128], [int(num_nodes), int(num_nodes*0.5), 128] ] # conservative as in paper vs deeper vs large vertically and deep\n",
        "      for layer_config in layer_configs:\n",
        "        params[\"layer_config\"] = layer_config\n",
        "        train_test_loop_optim(emb_method=\"sdne\", emb_size= layer_config[-1], epochs= 15, emb_params = params)\n",
        "\n",
        "#optimize_embeddings()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}